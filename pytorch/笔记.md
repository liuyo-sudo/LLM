<!-- TOC -->
  * [pytorch学习笔记](#pytorch学习笔记)
    * [张量](#张量)
    * [秩](#秩)
    * [pytorch训练流程](#pytorch训练流程)
    * [梯度](#梯度)
    * [损失函数](#损失函数)
<!-- TOC -->
## pytorch学习笔记

### 张量

### 秩
    使用 torch.svd 或 torch.qr 快速计算秩
```python
import torch
# 定义矩阵
A = torch.tensor([[1., 2., 3.], [2., 4., 6.], [1., 1., 0.]])

# 方法 1：SVD 计算秩
_, S, _ = torch.svd(A)
rank = torch.sum(S > 1e-5).item()
print(f"通过 SVD 计算的秩: {rank}")

# 方法 2：行简化（QR 分解近似）
Q, R = torch.qr(A)
non_zero_rows = torch.sum(torch.abs(torch.diag(R)) > 1e-5).item()
print(f"通过 QR 分解计算的秩: {non_zero_rows}")


import numpy as np
A = np.array([[1, 2, 3], [2, 4, 6], [1, 1, 0]])
_, s, _ = np.linalg.svd(A)
rank = np.sum(s > 1e-10)  # 非零奇异值数量
print(rank)  # 输出：2
```
### pytorch训练流程
    1、数据准备  D
    
    2、定义模型 M
    
    3、设置损失函数和优化器 L
    
    4、训练循环 T
    
    5、验证/评估 V
    
    6、保存和加载模型 S
    
    7、测试和推理 T

### 梯度
梯队下降：通过计算损失函数与模型参数的梯度，沿着梯队反方向不断更新参数，以逐渐减少损失

    1、初始化参数
    
    2、前向传播
    
    3、计算损失
    
    4、计算梯度

    通过反向传播（backpropagation），计算损失函数对每个参数的偏导数（梯度）。
    在PyTorch中，调用loss.backward()自动完成。

5、更新参数

![](E:\code\LLM\pytorch\image\屏幕截图 2025-06-13 144956.png)
6、迭代

## 损失函数

### 均方误差（MSE）

    torch.nn.MSELoss

    主要用于回归任务

### 交叉熵损失（Cross-Entropy Loss）

    torch.nn.CrossEntropyLoss
    用于分类任务
### 二元交叉熵
   
### 多标签分类
## 优化器

优化器负责根据梯度更新模型参数

<table>
    <tr>
        <th>名称</th>
        <th>原理</th>
        <th>特点和缺点</th>
        <th>场景</th>
    </tr>
</table>
## 学习率
控制参数更新的步长



