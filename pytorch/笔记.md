## pytorch学习笔记

### 张量

### 秩
    使用 torch.svd 或 torch.qr 快速计算秩
```python
import torch
# 定义矩阵
A = torch.tensor([[1., 2., 3.], [2., 4., 6.], [1., 1., 0.]])

# 方法 1：SVD 计算秩
_, S, _ = torch.svd(A)
rank = torch.sum(S > 1e-5).item()
print(f"通过 SVD 计算的秩: {rank}")

# 方法 2：行简化（QR 分解近似）
Q, R = torch.qr(A)
non_zero_rows = torch.sum(torch.abs(torch.diag(R)) > 1e-5).item()
print(f"通过 QR 分解计算的秩: {non_zero_rows}")


import numpy as np
A = np.array([[1, 2, 3], [2, 4, 6], [1, 1, 0]])
_, s, _ = np.linalg.svd(A)
rank = np.sum(s > 1e-10)  # 非零奇异值数量
print(rank)  # 输出：2
```
### pytorch训练流程
- 1、数据准备  D
- 2、定义模型 M
- 3、设置损失函数和优化器 L
- 4、训练循环 T
- 5、验证/评估 V
- 6、保存和加载模型 S
- 7、测试和推理 T

### 梯度
梯队下降：通过计算损失函数与模型参数的梯度，沿着梯队反方向不断更新参数，以逐渐减少损失

- 1、初始化参数
- 2、前向传播
- 3、计算损失
- 4、计算梯度
- 5、更新参数
- 6、迭代


### 优化器

- SGD 随机梯度下降 介绍: SGD 是最基本的优化算法，通过梯度更新参数，带有可选的动量机制以加速收敛。 
 - 公式:
   - 基本更新:
     - $\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta g(\theta_t) $
   - 带动量:
     - $v_{t+1} = \mu v_t + \nabla_\theta g(\theta_t) $
     - $\theta_{t+1} = \theta_t - \eta v_{t+1} $

  - 计算过程:
      - 计算当前参数 $\theta_t$ 的梯度 $\nabla_\theta J(\theta_t)$。
      - 若使用动量，更新速度 $v_{t+1}$，结合历史梯度。
      - 更新参数 $\theta_{t+1}$，其中 $\eta$ 是学习率，$\mu$ 是动量系数。

  - 特点: 简单高效，适合凸优化问题，但对非凸问题可能陷入局部极值
- Adadelta
  - 介绍: 
    - Adadelta 是 Adagrad 的改进，通过限制历史梯度的累积来避免学习率过快衰减，适合非平稳目标。
  - 公式:
      - 梯度平方期望:
      $E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho) g_t^2 $
      - 参数更新量:
      $\Delta \theta_t = -\frac{\sqrt{E[\Delta \theta^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t $
      - 参数更新:
      $\theta_{t+1} = \theta_t + \Delta \theta_t $
      - 更新量期望:
      $E[\Delta \theta^2]_t = \rho E[\Delta \theta^2]_{t-1} + (1-\rho) \Delta \theta_t^2 $

  - 计算过程:
    - 计算当前梯度 $g_t = \nabla_\theta J(\theta_t)$。
    - 更新梯度平方的指数移动平均 $E[g^2]_t$。
    - 计算更新量 $\Delta \theta_t$，基于历史更新量和梯度。
    - 更新参数 $\theta_{t+1}$，并更新 $E[\Delta \theta^2]_t$。
  - 特点: 无需手动设置学习率，适合处理稀疏数据。
- Adam
  - 介绍: 
     - Adam 结合动量法和 RMSprop，适应性强，广泛应用于深度学习。
  - 公式:
    - 一阶动量:
      - $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t $
    - 二阶动量:
      - $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 $
    - 偏差修正:
      - $\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} $
    - 参数更新:
      - $\theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} $
  - 计算过程:
    计算梯度 $g_t$。
    - 更新一阶动量 $m_t$ 和二阶动量 $v_t$。
    - 进行偏差修正，得到 $\hat{m}_t$ 和 $\hat{v}_t$。
    - 使用自适应学习率更新参数。
    - 特点: 收敛快，适合非平稳目标。
- AdamW
  - 介绍: AdamW 是 Adam 的正则化版本，通过解耦权重衰减提高泛化能力。
  - 公式:
    - 同 Adam，但参数更新加入权重衰减:
    - $\theta_{t+1} = \theta_t - \eta \cdot \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t \right) $
  - 计算过程:
    - 同 Adam 计算 $m_t$, $v_t$, $\hat{m}_t$, $\hat{v}_t$。
    - 在更新中加入权重衰减项 $\lambda \theta_t$。
  - 特点: 比 Adam 更适合需要正则化的任务。
- AMSGrad
- RMSProp
  - 介绍: RMSprop 通过指数移动平均调整学习率，解决 Adagrad 学习率衰减问题。
  - 公式:
    - 梯度平方期望:
      - $E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho) g_t^2 $
    - 参数更新:
      - $\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t $
  - 计算过程:
    - 计算梯度 $g_t$。
    - 更新 $E[g^2]_t$。
    - 使用自适应学习率更新参数。
  - 特点: 适合非平稳目标，收敛较快。
- Adafactor
- RMSprop
  - 介绍: RMSprop 通过指数移动平均调整学习率，解决 Adagrad 学习率衰减问题。
  - 公式:
    - 梯度平方期望:
      - $E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho) g_t^2 $
  - 参数更新:
    - $\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \cdot g_t $
    - 计算过程:
      - 计算梯度 $g_t$。
      - 更新 $E[g^2]_t$。
      - 使用自适应学习率更新参数。
    - 特点: 适合非平稳目标，收敛较快。
### 归一化
- LayerNorm  层归一化 用于Transformmer
- RMSNorm 均方根归一化，收敛优于LayerNorm，速度快, 用于Transformmer
- 
### 激活函数
- sigmoid 二分类
- tanh 双曲正切函数
- Relu 修正线性单元
- Leaky Relu 
- PReLU
- ELU
- GELU
  - GELU(x)=0.5∗x∗(1+Tanh( 
2/π
​
 ∗(x+0.044715∗x 
3
 )))
- SiLU
- Softmax
- Softplus
  - ln(1 + ex)
- Mish
  - Mish(x)=x∗Tanh(Softplus(x))

### Autograd
PyTorch 的 **Autograd** 功能是自动微分引擎，用于：
- **自动跟踪**：对 `requires_grad=True` 的张量操作，动态构建计算图。
- **自动梯度计算**：通过 `.backward()` 手动触发反向传播，基于链式法则计算梯度。
- **[灵活控制]()**：支持 `torch.no_grad()`、`.detach()` 等禁用或调整梯度跟踪。
